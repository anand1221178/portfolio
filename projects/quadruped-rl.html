<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes">
    <meta name="description" content="Robust Quadruped Reinforcement Learning - Systematic ablation study of SR2L and Domain Randomization.">
    <meta name="theme-color" content="#0a0a0b">
    <title>Robust Quadruped RL | Anand Patel</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../index.html" class="nav-logo">Anand<span>.</span></a>
            <div class="nav-links">
                <a href="../index.html#about">About</a>
                <a href="../index.html#projects">Projects</a>
                <a href="../index.html#contact">Contact</a>
            </div>
            <button class="nav-toggle" id="navToggle" aria-label="Toggle menu">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <!-- Mobile Menu -->
    <div class="mobile-menu" id="mobileMenu">
        <a href="../index.html#about">About</a>
        <a href="../index.html#projects">Projects</a>
        <a href="../index.html#contact">Contact</a>
    </div>

    <section class="project-hero">
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">Home</a>
                <i class="fas fa-chevron-right"></i>
                <a href="../index.html#projects">Projects</a>
                <i class="fas fa-chevron-right"></i>
                <span>Robust Quadruped RL</span>
            </div>
            <h1 class="project-title">Robust Quadruped Locomotion: Systematic Ablation of Robustness Techniques</h1>
            <div class="project-meta">
                <span><i class="fas fa-calendar"></i> 2024-2025</span>
                <span><i class="fas fa-tag"></i> Reinforcement Learning, Robotics</span>
                <span><i class="fas fa-clock"></i> 8 min read</span>
            </div>
        </div>
    </section>

    <section class="project-content">
        <div class="container">
            <div class="content-grid">
                <aside class="project-sidebar">
                    <div class="sidebar-section">
                        <h3>Technologies Used</h3>
                        <div class="tech-stack">
                            <span class="tech-item">Python</span>
                            <span class="tech-item">PPO</span>
                            <span class="tech-item">MuJoCo</span>
                            <span class="tech-item">Domain Randomization</span>
                            <span class="tech-item">SR2L</span>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Quick Stats</h3>
                        <div class="project-stats">
                            <div class="stat">
                                <span class="stat-value">38,000</span>
                                <span class="stat-label">Eval Episodes</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">5</span>
                                <span class="stat-label">Random Seeds</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">-9.4%</span>
                                <span class="stat-label">Negative Transfer</span>
                            </div>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Links</h3>
                        <div class="project-links">
                            <a href="https://github.com/anand1221178/robust-quadruped-rl" class="project-link" target="_blank">
                                <i class="fab fa-github"></i> View on GitHub
                            </a>
                            <a href="quadruped-rl-report.pdf" class="project-link" target="_blank">
                                <i class="fas fa-file-pdf"></i> Honours Thesis
                            </a>
                        </div>
                    </div>
                </aside>

                <main class="project-main">
                    <div class="project-overview">
                        <h2>Project Overview</h2>
                        <p>
                            This Honours thesis project provides a systematic ablation study of robustness techniques for quadruped locomotion.
                            Using the RealAnt environment, the study investigates the effects of Student-Teacher Robust RL (SR2L)
                            and Domain Randomization (DR) on the PPO algorithm. With over 38,000 evaluation episodes, the research
                            reveals a surprising "negative transfer" phenomenon.
                        </p>
                        <p>
                            The key finding is that combining SR2L and DR results in a 9.4% performance degradation compared to
                            using DR alone. This contradicts the common assumption that robustness techniques compose additively
                            and underscores the need for empirical validation in RL research.
                        </p>
                    </div>

                    <div class="project-section">
                        <h2>Key Features</h2>
                        <div class="feature-grid">
                            <div class="feature-card">
                                <i class="fas fa-vial"></i>
                                <h3>Systematic Ablation</h3>
                                <p>Isolated testing of PPO, SR2L, DR, and a combined SR2L+DR model.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-chart-line"></i>
                                <h3>Statistical Rigor</h3>
                                <p>38,000 evaluation episodes across 5 random seeds to ensure statistical significance.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-robot"></i>
                                <h3>RealAnt Environment</h3>
                                <p>MuJoCo-based quadruped with realistic physics and 8 actuated joints.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-exclamation-triangle"></i>
                                <h3>Negative Transfer Discovery</h3>
                                <p>The combined SR2L+DR approach underperformed the specialized DR-only model.</p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Implementation</h2>

                        <h3>1. Proximal Policy Optimization (PPO) Baseline</h3>
                        <p>
                            A standard PPO implementation with a clipped surrogate objective was used as the baseline.
                        </p>
                        <pre><code class="language-python"># From the PPO update rule
ratio = torch.exp(new_log_probs - old_log_probs)
surr1 = ratio * advantages
surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
actor_loss = -torch.min(surr1, surr2).mean()
</code></pre>

                        <h3>2. Domain Randomization (DR)</h3>
                        <p>
                            Physics parameters such as mass, friction, and gravity were randomized during training to improve the policy's robustness to variations in the environment.
                        </p>
                        <pre><code class="language-python"># From the Domain Randomization implementation
def randomize_environment(self):
    # Randomize mass, friction, damping, armature, and gravity
    mass_scale = np.random.uniform(*self.param_ranges['mass_scale'])
    self.env.model.body_mass[:] *= mass_scale
    # ... and so on for other parameters
</code></pre>

                        <h3>3. Student-Teacher Robust RL (SR2L)</h3>
                        <p>
                            A teacher network is trained to apply adversarial perturbations to the student's actions, forcing the student to learn a policy that is robust to worst-case scenarios.
                        </p>
                        <pre><code class="language-python"># From the SR2L training loop
# Student selects action
action = self.student.select_action(state)

# Teacher applies adversarial perturbation
perturbation = self.teacher.select_perturbation(state, action)
perturbed_action = action + perturbation

# ... student is updated to maximize reward, teacher to minimize it
</code></pre>
                    </div>

                    <div class="project-section">
                        <h2>Experimental Results</h2>
                        <div class="metrics-table">
                            <h3>Ablation Study Results (RealAnt Environment)</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>Mean Return</th>
                                        <th>Std Dev</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>PPO Baseline</td>
                                        <td>1247</td>
                                        <td>±183</td>
                                    </tr>
                                    <tr>
                                        <td>SR2L Only</td>
                                        <td>1589</td>
                                        <td>±142</td>
                                    </tr>
                                    <tr>
                                        <td><strong>DR Only</strong></td>
                                        <td><strong>1823</strong></td>
                                        <td><strong>±128</strong></td>
                                    </tr>
                                    <tr>
                                        <td>SR2L + DR Combined</td>
                                        <td>1651</td>
                                        <td>±156</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p style="margin-top: 1rem; font-size: 0.9rem; color: var(--text-secondary);">
                                <strong>Negative Transfer:</strong> The combined SR2L+DR approach scored 9.4% lower than the DR-only model, despite a larger evaluation budget.
                            </p>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Key Findings and Analysis</h2>
                        <div class="challenge-list">
                            <div class="challenge-item">
                                <h3><i class="fas fa-exclamation-circle"></i> Negative Transfer Phenomenon</h3>
                                <p>
                                    <strong>Discovery:</strong> The SR2L+DR model achieved a mean return of 1651, which is 9.4% lower than the 1823 return of the DR-only model.
                                </p>
                                <p>
                                    <strong>Root Cause Hypothesis:</strong> The adversarial perturbations from SR2L and the physics randomization from DR create conflicting training signals, preventing the policy from effectively learning a robust strategy.
                                </p>
                            </div>
                            <div class="challenge-item">
                                <h3><i class="fas fa-chart-line"></i> Domain Randomization's Superiority</h3>
                                <p>
                                    DR alone was the most effective method, producing the highest mean return and the lowest variance. This suggests that exposing the policy to a wide range of natural variations is more effective than adversarial training for this task.
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Implications for Sim-to-Real Transfer</h2>
                        <ul class="improvement-list">
                            <li>Robustness techniques do not always compose; in fact, they can interfere with each other.</li>
                            <li>Simple physics randomization can be more effective than complex adversarial training.</li>
                            <li>Empirical validation is crucial and can often contradict intuition.</li>
                            <li>Conflicting optimization objectives can significantly harm the learning process.</li>
                        </ul>
                    </div>

                    <div class="project-section">
                        <h2>Conclusion</h2>
                        <p>
                            This research provides strong evidence of "negative transfer" when combining robustness techniques in RL.
                            The systematic ablation study, backed by 38,000 evaluation episodes, demonstrates that Domain Randomization
                            alone is the most effective strategy for training a robust quadruped locomotion policy in the RealAnt environment.
                            This work highlights a critical lesson for the RL community: more techniques do not always lead to better performance.
                        </p>
                    </div>

                    <div class="navigation-buttons">
                        <a href="cuda-raytracing.html" class="btn btn-secondary">
                            <i class="fas fa-arrow-left"></i> Previous Project
                        </a>
                        <a href="matrix-multiplication.html" class="btn btn-primary">
                            Next Project <i class="fas fa-arrow-right"></i>
                        </a>
                    </div>
                </main>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="footer-inner">
            <p>&copy; 2025 Anand Patel. All rights reserved.</p>
            <div class="footer-links">
                <a href="https://github.com/anand1221178" target="_blank" aria-label="GitHub">
                    <i class="fab fa-github"></i>
                </a>
                <a href="https://linkedin.com/in/anand-patel1221" target="_blank" aria-label="LinkedIn">
                    <i class="fab fa-linkedin"></i>
                </a>
                <a href="mailto:anandpatel1221178@gmail.com" aria-label="Email">
                    <i class="fas fa-envelope"></i>
                </a>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
