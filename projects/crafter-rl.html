<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes">
    <title>Crafter RL with Intrinsic Curiosity - Anand's Portfolio</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">
                <a href="../index.html">Anand</a>
            </div>
            <div class="nav-menu" id="nav-menu">
                <a href="../index.html#home" class="nav-link">Home</a>
                <a href="../index.html#about" class="nav-link">About</a>
                <a href="../index.html#education" class="nav-link">Education</a>
                <a href="../index.html#projects" class="nav-link">Projects</a>
                <a href="../index.html#contact" class="nav-link">Contact</a>
            </div>
            <div class="nav-toggle" id="nav-toggle">
                <i class="fas fa-bars"></i>
            </div>
        </div>
    </nav>

    <section class="project-hero">
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">Home</a>
                <i class="fas fa-chevron-right"></i>
                <a href="../index.html#projects">Projects</a>
                <i class="fas fa-chevron-right"></i>
                <span>Crafter RL with ICM</span>
            </div>
            <h1 class="project-title">Crafter Reinforcement Learning: PPO with Intrinsic Curiosity Module</h1>
            <div class="project-meta">
                <span><i class="fas fa-calendar"></i> 2024-2025</span>
                <span><i class="fas fa-tag"></i> Reinforcement Learning, Exploration</span>
                <span><i class="fas fa-clock"></i> 6 min read</span>
            </div>
        </div>
    </section>

    <section class="project-content">
        <div class="container">
            <div class="content-grid">
                <aside class="project-sidebar">
                    <div class="sidebar-section">
                        <h3>Technologies Used</h3>
                        <div class="tech-stack">
                            <span class="tech-item">Python</span>
                            <span class="tech-item">PPO</span>
                            <span class="tech-item">ICM</span>
                            <span class="tech-item">Crafter</span>
                            <span class="tech-item">PyTorch</span>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Quick Stats</h3>
                        <div class="project-stats">
                            <div class="stat">
                                <span class="stat-value">8.61%</span>
                                <span class="stat-label">Final Score</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">+69.5%</span>
                                <span class="stat-label">Improvement</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">11</span>
                                <span class="stat-label">Experiments</span>
                            </div>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Links</h3>
                        <div class="project-links">
                            <a href="#" class="project-link">
                                <i class="fab fa-github"></i> View on GitHub
                            </a>
                            <a href="#" class="project-link">
                                <i class="fas fa-file-pdf"></i> Project Report
                            </a>
                        </div>
                    </div>
                </aside>

                <main class="project-main">
                    <div class="project-overview">
                        <h2>Project Overview</h2>
                        <p>
                            This project tackles the sparse reward challenge in Crafter, a procedurally-generated survival
                            environment, by implementing PPO with an Intrinsic Curiosity Module (ICM). The ICM provides
                            exploration bonuses based on prediction error, encouraging the agent to seek novel states
                            in the absence of external rewards.
                        </p>
                        <p>
                            Through 11 experiments (4 successful, 7 failed), the final PPO+ICM implementation achieved
                            8.61% score, representing a 69.5% improvement over the baseline. This demonstrates the effectiveness
                            of intrinsic motivation for exploration in sparse reward environments, though absolute performance
                            remains limited due to Crafter's complexity.
                        </p>
                    </div>

                    <div class="project-section">
                        <h2>Key Features</h2>
                        <div class="feature-grid">
                            <div class="feature-card">
                                <i class="fas fa-brain"></i>
                                <h3>Intrinsic Curiosity Module</h3>
                                <p>Forward model predicts next state features for exploration bonuses</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-eye"></i>
                                <h3>Feature Encoding</h3>
                                <p>Inverse model learns state-independent representations</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-chart-line"></i>
                                <h3>Dual Reward System</h3>
                                <p>Combines extrinsic environment rewards with intrinsic curiosity</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-vial"></i>
                                <h3>Iterative Experimentation</h3>
                                <p>11 experiments documenting failures and successes</p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Implementation</h2>

                        <h3>1. Proximal Policy Optimization (PPO)</h3>
                        <p>
                            Base PPO implementation for policy learning in Crafter:
                        </p>
                        <pre><code class="language-python">import torch
import torch.nn as nn
from torch.distributions import Categorical

class PPOPolicy(nn.Module):
    def __init__(self, obs_shape, action_dim):
        super().__init__()
        # CNN feature extractor for pixel observations
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten()
        )

        # Actor (policy) and Critic (value) heads
        self.actor = nn.Sequential(
            nn.Linear(3136, 512),
            nn.ReLU(),
            nn.Linear(512, action_dim)
        )

        self.critic = nn.Sequential(
            nn.Linear(3136, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self, obs):
        features = self.conv(obs)
        action_logits = self.actor(features)
        value = self.critic(features)
        return action_logits, value

# PPO training with clipped objective
# Baseline score: 5.08% (sparse rewards limit learning)</code></pre>

                        <h3>2. Intrinsic Curiosity Module (ICM)</h3>
                        <p>
                            ICM consists of forward and inverse models to generate intrinsic rewards:
                        </p>
                        <pre><code class="language-python">class ICM(nn.Module):
    def __init__(self, feature_dim=288, action_dim=17):
        super().__init__()

        # Feature encoder (shared)
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(3136, feature_dim)
        )

        # Inverse model: φ(s_t), φ(s_{t+1}) → a_t
        self.inverse_model = nn.Sequential(
            nn.Linear(feature_dim * 2, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )

        # Forward model: φ(s_t), a_t → φ(s_{t+1})
        self.forward_model = nn.Sequential(
            nn.Linear(feature_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, feature_dim)
        )

    def compute_intrinsic_reward(self, state, action, next_state):
        # Encode states
        phi_t = self.encoder(state)
        phi_t1 = self.encoder(next_state)

        # Forward model prediction
        action_onehot = torch.nn.functional.one_hot(action, num_classes=17).float()
        phi_t1_pred = self.forward_model(torch.cat([phi_t, action_onehot], dim=1))

        # Intrinsic reward = prediction error
        intrinsic_reward = 0.5 * (phi_t1 - phi_t1_pred).pow(2).mean(dim=1)

        return intrinsic_reward

# Intrinsic reward encourages exploration of novel states
# Higher prediction error → higher curiosity bonus</code></pre>

                        <h3>3. Combined PPO+ICM Training</h3>
                        <p>
                            Integration of extrinsic and intrinsic rewards:
                        </p>
                        <pre><code class="language-python">class PPO_ICM_Agent:
    def __init__(self, obs_shape, action_dim):
        self.policy = PPOPolicy(obs_shape, action_dim)
        self.icm = ICM(feature_dim=288, action_dim=action_dim)

        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        self.icm_optimizer = torch.optim.Adam(self.icm.parameters(), lr=1e-3)

        self.intrinsic_coef = 0.01  # Weight for intrinsic rewards

    def compute_total_reward(self, extrinsic_reward, state, action, next_state):
        # Compute intrinsic curiosity reward
        intrinsic_reward = self.icm.compute_intrinsic_reward(state, action, next_state)

        # Combine with extrinsic reward
        total_reward = extrinsic_reward + self.intrinsic_coef * intrinsic_reward

        return total_reward

    def train_step(self, trajectories):
        # 1. Update ICM (forward and inverse models)
        inverse_loss = self.icm.inverse_model_loss(trajectories)
        forward_loss = self.icm.forward_model_loss(trajectories)
        icm_loss = inverse_loss + forward_loss

        self.icm_optimizer.zero_grad()
        icm_loss.backward()
        self.icm_optimizer.step()

        # 2. Update PPO policy with combined rewards
        total_rewards = self.compute_total_reward(
            trajectories.extrinsic_rewards,
            trajectories.states,
            trajectories.actions,
            trajectories.next_states
        )

        ppo_loss = self.policy.ppo_loss(trajectories, total_rewards)

        self.policy_optimizer.zero_grad()
        ppo_loss.backward()
        self.policy_optimizer.step()

# Result: 8.61% score (+69.5% over 5.08% baseline)</code></pre>
                    </div>

                    <div class="project-section">
                        <h2>Experimental Results</h2>
                        <div class="metrics-table">
                            <h3>Experiment Progression (11 Total)</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Experiment</th>
                                        <th>Configuration</th>
                                        <th>Score</th>
                                        <th>Outcome</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>1-3</td>
                                        <td>PPO baseline variations</td>
                                        <td>4.2-5.1%</td>
                                        <td>Established baseline</td>
                                    </tr>
                                    <tr>
                                        <td>4-7</td>
                                        <td>ICM with incorrect hyperparams</td>
                                        <td>2.8-4.5%</td>
                                        <td>Failed (overpowered intrinsic)</td>
                                    </tr>
                                    <tr>
                                        <td>8-9</td>
                                        <td>Reduced intrinsic coefficient</td>
                                        <td>6.3-7.2%</td>
                                        <td>Improvement observed</td>
                                    </tr>
                                    <tr>
                                        <td>10-11</td>
                                        <td>Tuned ICM (intrinsic_coef=0.01)</td>
                                        <td>8.4-8.6%</td>
                                        <td><strong>Success (+69.5%)</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                            <p style="margin-top: 1rem; font-size: 0.9rem; color: var(--text-secondary);">
                                <strong>Final Result:</strong> PPO+ICM achieved 8.61% vs 5.08% baseline (69.5% relative improvement)
                            </p>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Key Insights and Challenges</h2>
                        <div class="challenge-list">
                            <div class="challenge-item">
                                <h3><i class="fas fa-exclamation-circle"></i> Hyperparameter Sensitivity</h3>
                                <p>
                                    <strong>Challenge:</strong> Experiments 4-7 failed because intrinsic rewards dominated
                                    extrinsic rewards (intrinsic_coef=0.1 was too high), causing the agent to ignore
                                    actual task objectives in favor of pure exploration.
                                </p>
                                <p>
                                    <strong>Solution:</strong> Reduced intrinsic_coef to 0.01, balancing exploration
                                    (ICM) with exploitation (environment rewards). This 10× reduction enabled successful learning.
                                </p>
                            </div>
                            <div class="challenge-item">
                                <h3><i class="fas fa-chart-line"></i> Sparse Rewards Limit Performance</h3>
                                <p>
                                    Crafter's sparse reward structure (achievements like "collect wood", "defeat zombie")
                                    makes learning difficult even with ICM. Absolute score of 8.61% remains low, indicating
                                    the agent still struggles with long-horizon planning and credit assignment.
                                </p>
                            </div>
                            <div class="challenge-item">
                                <h3><i class="fas fa-vial"></i> Value of Failed Experiments</h3>
                                <p>
                                    Documenting 7 failed experiments was crucial for understanding ICM behavior. Failures
                                    revealed that excessive intrinsic motivation creates "exploration addiction" where
                                    the agent ignores task-relevant rewards - a key insight for future work.
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Contributions</h2>
                        <ul class="improvement-list">
                            <li><strong>PPO+ICM integration:</strong> Successfully combined policy gradient method with curiosity-driven exploration</li>
                            <li><strong>69.5% improvement:</strong> Demonstrated ICM's effectiveness for sparse reward environments</li>
                            <li><strong>Hyperparameter discovery:</strong> Found optimal intrinsic_coef=0.01 through systematic experimentation</li>
                            <li><strong>Documented failures:</strong> 7 failed experiments provide insights into common pitfalls</li>
                            <li><strong>Feature encoding:</strong> Shared CNN encoder between ICM and policy reduces computational cost</li>
                        </ul>
                    </div>

                    <div class="project-section">
                        <h2>Conclusion</h2>
                        <p>
                            This project demonstrates that Intrinsic Curiosity Modules can significantly improve exploration
                            in sparse reward environments, achieving 69.5% performance gain over vanilla PPO in Crafter.
                            However, the absolute score of 8.61% highlights Crafter's difficulty and the limitations of
                            current exploration methods for complex, long-horizon tasks.
                        </p>
                        <p>
                            The 11 experiments (including 7 documented failures) reveal critical insights about balancing
                            intrinsic and extrinsic rewards. Setting intrinsic_coef too high causes "exploration addiction"
                            where agents ignore task objectives, while too low provides insufficient exploration bonus.
                            This balance is environment-specific and requires careful tuning - a key lesson for practical
                            RL applications.
                        </p>
                    </div>

                    <div class="navigation-buttons">
                        <a href="bst-os-tree.html" class="btn btn-secondary">
                            <i class="fas fa-arrow-left"></i> Previous Project
                        </a>
                        <a href="../index.html#projects" class="btn btn-primary">
                            Back to Projects <i class="fas fa-arrow-right"></i>
                        </a>
                    </div>
                </main>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Anand. All rights reserved.</p>
            <div class="social-links">
                <a href="https://github.com/anand1221178" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/anand-patel1221/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:anandpatel1221178@gmail.com"><i class="fas fa-envelope"></i></a>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
