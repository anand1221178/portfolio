<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes">
    <meta name="description" content="Parallel K-Nearest Neighbors with OpenMP and Federated Learning exploration.">
    <meta name="theme-color" content="#0a0a0b">
    <title>Parallel KNN | Anand Patel</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <nav class="nav">
        <div class="nav-inner">
            <div class="nav-logo">
                <a href="../index.html">Anand<span>.</span></a>
            </div>
            <div class="nav-links" id="nav-menu">
                <a href="../index.html#home" class="nav-link">Home</a>
                <a href="../index.html#about" class="nav-link">About</a>
                <a href="../index.html#education" class="nav-link">Education</a>
                <a href="../index.html#projects" class="nav-link">Projects</a>
                <a href="../index.html#contact" class="nav-link">Contact</a>
            </div>
            <button class="nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
                <i class="fas fa-bars"></i>
            </button>
        </div>
    </nav>

    <section class="project-hero">
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">Home</a>
                <i class="fas fa-chevron-right"></i>
                <a href="../index.html#projects">Projects</a>
                <i class="fas fa-chevron-right"></i>
                <span>Parallel K-Nearest Neighbors</span>
            </div>
            <h1 class="project-title">Parallel K-Nearest Neighbors with OpenMP and Federated Learning Comparison</h1>
            <div class="project-meta">
                <span><i class="fas fa-calendar"></i> 2024-2025</span>
                <span><i class="fas fa-tag"></i> High Performance Computing, Distributed Systems</span>
                <span><i class="fas fa-clock"></i> 7 min read</span>
            </div>
        </div>
    </section>

    <section class="project-content">
        <div class="container">
            <div class="content-grid">
                <aside class="project-sidebar">
                    <div class="sidebar-section">
                        <h3>Technologies Used</h3>
                        <div class="tech-stack">
                            <span class="tech-item">C++</span>
                            <span class="tech-item">OpenMP</span>
                            <span class="tech-item">MPI</span>
                            <span class="tech-item">Federated Learning</span>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Quick Stats</h3>
                        <div class="project-stats">
                            <div class="stat">
                                <span class="stat-value">17.27x</span>
                                <span class="stat-label">Speedup</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">81.97%</span>
                                <span class="stat-label">FL Accuracy</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">2.14x</span>
                                <span class="stat-label">vs Centralized</span>
                            </div>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Links</h3>
                        <div class="project-links">
                            <a href="https://github.com/anand1221178/hpc-knn-federated" class="project-link" target="_blank">
                                <i class="fab fa-github"></i> View on GitHub
                            </a>
                            <a href="parallel-knn-report.pdf" class="project-link" target="_blank">
                                <i class="fas fa-file-pdf"></i> Performance Analysis
                            </a>
                        </div>
                    </div>
                </aside>

                <main class="project-main">
                    <div class="project-overview">
                        <h2>Project Overview</h2>
                        <p>
                            This project explores parallel and distributed machine learning by implementing a K-Nearest Neighbors (KNN)
                            classifier with OpenMP and comparing centralized vs. federated learning approaches with MPI. The parallel
                            KNN implementation achieves a 17.27× speedup on 128 threads by optimizing the distance calculation and
                            neighbor search algorithms.
                        </p>
                        <p>
                            The federated learning component demonstrates superior performance on heterogeneous data distributions,
                            achieving 81.97% accuracy compared to just 38.25% for a centralized training approach on the same
                            non-IID data. This highlights the critical importance of data locality in real-world distributed
                            machine learning systems.
                        </p>
                    </div>

                    <div class="project-section">
                        <h2>Key Features</h2>
                        <div class="feature-grid">
                            <div class="feature-card">
                                <i class="fas fa-project-diagram"></i>
                                <h3>OpenMP Parallelization</h3>
                                <p>Achieved a 17.27× speedup on 128 threads using OpenMP sections for work-sharing.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-network-wired"></i>
                                <h3>Federated Learning</h3>
                                <p>Implemented a federated averaging algorithm with MPI for distributed training.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-database"></i>
                                <h3>Non-IID Data Handling</h3>
                                <p>Showcased federated learning's superior performance on heterogeneous data distributions.</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-chart-line"></i>
                                <h3>Hybrid Parallelism</h3>
                                <p>Combined OpenMP and MPI for a multi-level parallelization strategy.</p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Implementation</h2>

                        <h3>1. OpenMP Parallelization of KNN</h3>
                        <p>
                            The KNN algorithm was parallelized using OpenMP. The "sections" work-sharing construct provided the best performance due to the coarse-grained nature of the KNN pipeline.
                        </p>
                        <pre><code class="language-cpp">// From knn.cpp
void knn_classify_sections(const Dataset& data, int k, int num_threads) {
    #pragma omp parallel sections num_threads(num_threads)
    {
        #pragma omp section
        {
            // Compute distances for a subset of the data
        }
        #pragma omp section
        {
            // Find k-nearest neighbors for another subset
        }
        // ... and so on
    }
}</code></pre>

                        <h3>2. Federated Learning with MPI</h3>
                        <p>
                            A federated learning system was implemented using MPI to simulate a distributed training scenario. The federated averaging algorithm was used to aggregate model weights from multiple workers.
                        </p>
                        <pre><code class="language-cpp">// From the federated learning implementation
void federated_learning_mpi(...) {
    for (int round = 0; round < num_rounds; round++) {
        // Train local model on local data
        local_model.train(local_data);

        // Aggregate model weights using MPI_Allreduce
        MPI_Allreduce(
            local_model.get_weights().data(),
            global_weights.data(),
            // ...
        );

        // Update local model with averaged global weights
        local_model.set_weights(global_weights);
    }
}</code></pre>
                    </div>

                    <div class="project-section">
                        <h2>Performance Analysis</h2>
                        <div class="metrics-table">
                            <h3>OpenMP KNN Scalability</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Threads</th>
                                        <th>Time (s)</th>
                                        <th>Speedup</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>1</td>
                                        <td>89.32</td>
                                        <td>1.0x</td>
                                    </tr>
                                    <tr>
                                        <td>16</td>
                                        <td>9.12</td>
                                        <td>9.79x</td>
                                    </tr>
                                    <tr>
                                        <td>64</td>
                                        <td>5.38</td>
                                        <td>16.60x</td>
                                    </tr>
                                    <tr>
                                        <td><strong>128</strong></td>
                                        <td><strong>5.17</strong></td>
                                        <td><strong>17.27x</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <div class="metrics-table" style="margin-top: 2rem;">
                            <h3>Centralized vs. Federated Learning on Non-IID Data</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Approach</th>
                                        <th>Accuracy</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Federated Learning (MPI)</strong></td>
                                        <td><strong>81.97%</strong></td>
                                    </tr>
                                    <tr>
                                        <td>Centralized Training</td>
                                        <td>38.25%</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Conclusion</h2>
                        <p>
                            This project successfully demonstrates the application of parallel and distributed computing techniques
                            to machine learning algorithms. The parallel KNN implementation achieved a significant 17.27x speedup
                            with OpenMP. Furthermore, the federated learning component highlighted the robustness of distributed
                            learning on non-IID data, outperforming a centralized approach by a factor of 2.14x. These results
                            underscore the importance of choosing the right parallelization strategy and data distribution model
                            for real-world machine learning applications.
                        </p>
                    </div>

                    <div class="navigation-buttons">
                        <a href="matrix-multiplication.html" class="btn btn-secondary">
                            <i class="fas fa-arrow-left"></i> Previous Project
                        </a>
                        <a href="bst-os-tree.html" class="btn btn-primary">
                            Next Project <i class="fas fa-arrow-right"></i>
                        </a>
                    </div>
                </main>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Anand. All rights reserved.</p>
            <div class="social-links">
                <a href="https://github.com/anand1221178" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/anand-patel1221/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:anandpatel1221178@gmail.com"><i class="fas fa-envelope"></i></a>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
</body>
</html>
