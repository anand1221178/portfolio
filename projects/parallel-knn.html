<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes">
    <title>Parallel K-Nearest Neighbors - Anand's Portfolio</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/project.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">
                <a href="../index.html">Anand</a>
            </div>
            <div class="nav-menu" id="nav-menu">
                <a href="../index.html#home" class="nav-link">Home</a>
                <a href="../index.html#about" class="nav-link">About</a>
                <a href="../index.html#education" class="nav-link">Education</a>
                <a href="../index.html#projects" class="nav-link">Projects</a>
                <a href="../index.html#contact" class="nav-link">Contact</a>
            </div>
            <div class="nav-toggle" id="nav-toggle">
                <i class="fas fa-bars"></i>
            </div>
        </div>
    </nav>

    <section class="project-hero">
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">Home</a>
                <i class="fas fa-chevron-right"></i>
                <a href="../index.html#projects">Projects</a>
                <i class="fas fa-chevron-right"></i>
                <span>Parallel K-Nearest Neighbors</span>
            </div>
            <h1 class="project-title">Parallel K-Nearest Neighbors with OpenMP and Federated Learning Comparison</h1>
            <div class="project-meta">
                <span><i class="fas fa-calendar"></i> 2024-2025</span>
                <span><i class="fas fa-tag"></i> High Performance Computing, Distributed Systems</span>
                <span><i class="fas fa-clock"></i> 7 min read</span>
            </div>
        </div>
    </section>

    <section class="project-content">
        <div class="container">
            <div class="content-grid">
                <aside class="project-sidebar">
                    <div class="sidebar-section">
                        <h3>Technologies Used</h3>
                        <div class="tech-stack">
                            <span class="tech-item">C++</span>
                            <span class="tech-item">OpenMP</span>
                            <span class="tech-item">Machine Learning</span>
                            <span class="tech-item">Parallel Computing</span>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Quick Stats</h3>
                        <div class="project-stats">
                            <div class="stat">
                                <span class="stat-value">17.27x</span>
                                <span class="stat-label">Speedup (128 threads)</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">81.97%</span>
                                <span class="stat-label">Fed. Learning Acc</span>
                            </div>
                            <div class="stat">
                                <span class="stat-value">OpenMP+MPI</span>
                                <span class="stat-label">Hybrid Parallel</span>
                            </div>
                        </div>
                    </div>
                    <div class="sidebar-section">
                        <h3>Links</h3>
                        <div class="project-links">
                            <a href="#" class="project-link">
                                <i class="fab fa-github"></i> View on GitHub
                            </a>
                            <a href="#" class="project-link">
                                <i class="fas fa-chart-bar"></i> Performance Analysis
                            </a>
                        </div>
                    </div>
                </aside>

                <main class="project-main">
                    <div class="project-overview">
                        <h2>Project Overview</h2>
                        <p>
                            This project implements parallel KNN classification using OpenMP and compares centralized
                            vs federated learning approaches using MPI for distributed training. The parallel KNN
                            implementation achieves 17.27× speedup with 128 threads through optimized distance calculations
                            and neighbor search algorithms.
                        </p>
                        <p>
                            The federated learning component demonstrates superior performance on heterogeneous data
                            distributions, achieving 81.97% accuracy compared to 38.25% for centralized training on
                            non-IID partitions. This highlights the importance of data locality in distributed machine
                            learning systems.
                        </p>
                    </div>

                    <div class="project-section">
                        <h2>Key Features</h2>
                        <div class="feature-grid">
                            <div class="feature-card">
                                <i class="fas fa-project-diagram"></i>
                                <h3>OpenMP Parallelization</h3>
                                <p>17.27× speedup with 128 threads using sections and tasks work-sharing</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-network-wired"></i>
                                <h3>Federated Learning</h3>
                                <p>MPI-based distributed training with federated averaging</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-database"></i>
                                <h3>Non-IID Data Handling</h3>
                                <p>Superior performance on heterogeneous data distributions</p>
                            </div>
                            <div class="feature-card">
                                <i class="fas fa-chart-line"></i>
                                <h3>Hybrid Parallelism</h3>
                                <p>Combined OpenMP+MPI for multi-level parallelization</p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Implementation</h2>

                        <h3>1. OpenMP Parallelization Strategies</h3>
                        <p>
                            Compared three OpenMP work-sharing approaches - sections outperformed tasks and for loops:
                        </p>
                        <pre><code class="language-cpp">#include <omp.h>
#include <vector>

// Best performing: sections work-sharing (17.27x speedup at 128 threads)
void knn_classify_sections(const Dataset& data, int k, int num_threads) {
    #pragma omp parallel sections num_threads(num_threads)
    {
        #pragma omp section
        {
            // Distance calculation for subset 1
            compute_distances_euclidean(data, subset1);
        }
        #pragma omp section
        {
            // Distance calculation for subset 2
            compute_distances_euclidean(data, subset2);
        }
        #pragma omp section
        {
            // K-smallest selection
            find_k_nearest_neighbors(distances, k);
        }
        #pragma omp section
        {
            // Majority voting
            classify_by_voting(nearest_neighbors);
        }
    }
}

// Sections outperformed tasks due to lower scheduling overhead
// for coarse-grained parallel regions in KNN workflow</code></pre>

                        <h3>2. Federated Learning with MPI</h3>
                        <p>
                            Distributed training using federated averaging outperformed centralized approaches on non-IID data:
                        </p>
                        <pre><code class="language-cpp">#include <mpi.h>

// Federated averaging: 81.97% accuracy vs 38.25% centralized on non-IID data
void federated_learning_mpi(Dataset& local_data, int num_rounds, int rank, int size) {
    Model local_model;

    for (int round = 0; round < num_rounds; round++) {
        // Each worker trains on local non-IID partition
        local_model.train(local_data, epochs=1);

        // Get local model weights
        std::vector<float> local_weights = local_model.get_weights();
        std::vector<float> global_weights(local_weights.size());

        // Federated averaging: aggregate weights across all workers
        MPI_Allreduce(
            local_weights.data(),
            global_weights.data(),
            local_weights.size(),
            MPI_FLOAT,
            MPI_SUM,
            MPI_COMM_WORLD
        );

        // Average by number of workers
        for (auto& w : global_weights) {
            w /= size;
        }

        // Update local model with global weights
        local_model.set_weights(global_weights);
    }
}

// Key insight: Federated learning preserves data locality, crucial for
// heterogeneous (non-IID) distributions common in real-world scenarios</code></pre>

                        <h3>3. Performance Comparison: Centralized vs Federated</h3>
                        <p>
                            On heterogeneous (non-IID) MNIST data, federated learning dramatically outperformed centralized:
                        </p>
                        <div class="metrics-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Approach</th>
                                        <th>Accuracy</th>
                                        <th>Notes</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Federated Learning (MPI)</strong></td>
                                        <td><strong>81.97%</strong></td>
                                        <td>4 workers, non-IID partitions</td>
                                    </tr>
                                    <tr>
                                        <td>Centralized Training</td>
                                        <td>38.25%</td>
                                        <td>Same data, pooled centrally</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p style="margin-top: 1rem;">
                            <strong>Key Finding:</strong> When data is heterogeneously distributed (non-IID), federated
                            learning's local training + global aggregation preserves data characteristics better than
                            naive centralized pooling, resulting in 2.14× better accuracy.
                        </p>
                    </div>

                    <div class="project-section">
                        <h2>Performance Analysis</h2>
                        <div class="metrics-table">
                            <h3>OpenMP KNN Scalability (MNIST Dataset)</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>Threads</th>
                                        <th>Time (seconds)</th>
                                        <th>Speedup</th>
                                        <th>Work-Sharing</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>1</td>
                                        <td>89.32</td>
                                        <td>1.0x</td>
                                        <td>Baseline (sequential)</td>
                                    </tr>
                                    <tr>
                                        <td>2</td>
                                        <td>52.47</td>
                                        <td>1.70x</td>
                                        <td rowspan="7">Sections (best)</td>
                                    </tr>
                                    <tr>
                                        <td>4</td>
                                        <td>28.19</td>
                                        <td>3.17x</td>
                                    </tr>
                                    <tr>
                                        <td>8</td>
                                        <td>15.84</td>
                                        <td>5.64x</td>
                                    </tr>
                                    <tr>
                                        <td>16</td>
                                        <td>9.12</td>
                                        <td>9.79x</td>
                                    </tr>
                                    <tr>
                                        <td>32</td>
                                        <td>6.45</td>
                                        <td>13.85x</td>
                                    </tr>
                                    <tr>
                                        <td>64</td>
                                        <td>5.38</td>
                                        <td>16.60x</td>
                                    </tr>
                                    <tr>
                                        <td><strong>128</strong></td>
                                        <td><strong>5.17</strong></td>
                                        <td><strong>17.27x</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                            <p style="margin-top: 1rem; font-size: 0.9rem; color: var(--text-secondary);">
                                Sections work-sharing outperformed tasks due to lower scheduling overhead for coarse-grained parallelism
                            </p>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Key Findings and Insights</h2>
                        <div class="challenge-list">
                            <div class="challenge-item">
                                <h3><i class="fas fa-bolt"></i> OpenMP Work-Sharing Comparison</h3>
                                <p>
                                    <strong>Sections (17.27×) > Tasks (14.52×) > For loops (11.83×)</strong><br>
                                    Sections won due to lower scheduling overhead for coarse-grained parallel regions.
                                    Tasks incurred higher runtime overhead from dynamic task creation, while for loops
                                    had load imbalance issues in the KNN classification pipeline.
                                </p>
                            </div>
                            <div class="challenge-item">
                                <h3><i class="fas fa-network-wired"></i> Federated Learning Advantage</h3>
                                <p>
                                    <strong>81.97% (Federated) vs 38.25% (Centralized) on non-IID MNIST</strong><br>
                                    Centralized training on heterogeneous data pools mismatched distributions, causing
                                    catastrophic accuracy loss. Federated learning preserved local data characteristics
                                    through distributed training + global aggregation, achieving 2.14× better accuracy.
                                </p>
                            </div>
                            <div class="challenge-item">
                                <h3><i class="fas fa-project-diagram"></i> Scalability Saturation</h3>
                                <p>
                                    Speedup saturated at 128 threads (17.27×) due to Amdahl's Law - sequential portions
                                    (k-nearest selection, voting) became bottleneck. Diminishing returns beyond 64 threads
                                    suggest optimal configuration for MNIST-scale KNN workloads.
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="project-section">
                        <h2>Technical Contributions</h2>
                        <ul class="improvement-list">
                            <li><strong>17.27× OpenMP speedup:</strong> Sections work-sharing outperformed tasks and for loops</li>
                            <li><strong>Federated learning wins on non-IID data:</strong> 81.97% vs 38.25% centralized accuracy</li>
                            <li><strong>MPI Allreduce for weight aggregation:</strong> Efficient federated averaging across workers</li>
                            <li><strong>Hybrid OpenMP+MPI parallelism:</strong> Multi-level parallelization for distributed KNN</li>
                            <li><strong>Scalability analysis:</strong> Identified Amdahl's Law bottleneck at 128 threads</li>
                        </ul>
                    </div>

                    <div class="project-section">
                        <h2>Conclusion</h2>
                        <p>
                            This project achieved 17.27× parallel speedup for KNN classification using OpenMP sections
                            work-sharing and demonstrated that federated learning outperforms centralized training on
                            non-IID data (81.97% vs 38.25% accuracy). The comparison of OpenMP strategies (sections, tasks,
                            for loops) revealed that coarse-grained parallelism minimizes scheduling overhead.
                        </p>
                        <p>
                            The federated learning component using MPI highlights the critical importance of preserving
                            data locality in distributed systems. When data is heterogeneously distributed, local training
                            with global aggregation significantly outperforms naive centralized pooling - a key insight for
                            real-world distributed machine learning applications.
                        </p>
                    </div>

                    <div class="navigation-buttons">
                        <a href="investment-tracker.html" class="btn btn-secondary">
                            <i class="fas fa-arrow-left"></i> Previous Project
                        </a>
                        <a href="ciso-api.html" class="btn btn-primary">
                            Next Project <i class="fas fa-arrow-right"></i>
                        </a>
                    </div>
                </main>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Anand. All rights reserved.</p>
            <div class="social-links">
                <a href="https://github.com/anand1221178" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/anand-patel1221/" target="_blank"><i class="fab fa-linkedin"></i></a>
                <a href="mailto:anandpatel1221178@gmail.com"><i class="fas fa-envelope"></i></a>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
</body>
</html>
